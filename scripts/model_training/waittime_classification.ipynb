{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "\n",
    "# Add project root to the Python path\n",
    "project_root = Path.cwd().parent.parent\n",
    "sys.path.append(str(project_root))\n",
    "import shap\n",
    "\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap\n",
    "from scipy import stats\n",
    "from scipy.sparse import issparse\n",
    "from sklearn.base import clone\n",
    "\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from imblearn.over_sampling import ADASYN, SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.ensemble import (RandomForestClassifier, ExtraTreesClassifier,\n",
    "                              GradientBoostingClassifier, GradientBoostingRegressor,\n",
    "                              RandomForestRegressor)\n",
    "from sklearn.feature_selection import (SelectFromModel, SelectKBest,\n",
    "                                       RFECV, RFE, mutual_info_classif,\n",
    "                                       mutual_info_regression)\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression, ElasticNet\n",
    "from sklearn.metrics import (classification_report, roc_auc_score, precision_recall_curve, auc,\n",
    "                             accuracy_score, confusion_matrix, explained_variance_score,\n",
    "                             f1_score, mean_absolute_error, mean_squared_error, precision_recall_fscore_support,\n",
    "                             precision_score, r2_score, recall_score, silhouette_score)\n",
    "from sklearn.model_selection import (train_test_split, RandomizedSearchCV, GridSearchCV, StratifiedKFold)\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import (StandardScaler, OneHotEncoder, MinMaxScaler, FunctionTransformer, LabelEncoder, OrdinalEncoder)\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from umap import UMAP\n",
    "from xgboost import XGBClassifier\n",
    "from tempfile import gettempdir\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, auc, matthews_corrcoef\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from src.data_management.data_preprocessing_pipeline import DataPreprocessingPipeline\n",
    "\n",
    "import scipy.sparse\n",
    "import config\n",
    "from config import RANDOM_SEED\n",
    "config.set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading dataset\n",
    "path = \"/Users/jeancherizol/MADS/699/all_data\"\n",
    "path_models = \"/Users/jeancherizol/MADS/699/SIADS/emergency-dept-optimization/models\"\n",
    "emergency_df = pd.read_sas(f\"{path}/nhamcs14.sas7bdat\")\n",
    "\n",
    "target = 'WAITTIME_BINARY'\n",
    "target_to_drop = ['WAITTIME','LOV_BINARY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-Cleaning data...\n",
      "Data cleaning completed\n",
      "Size of Initial dataset:(23844, 1012)\n",
      "Size of cleaned dataset:(23844, 408)\n",
      "\n",
      "2-Applying feature engineering...\n",
      "Feature engineering completed\n",
      "Size of the dataset after feature engineering:(23844, 445)\n",
      "\n",
      "3-Splitting data...\n",
      "self.stratify: True\n",
      "Splitting data completed\n",
      "\n",
      "4-Loading data...\n",
      "train_df size: (19075, 445)\n",
      "X_train size: (19075, 444)\n",
      "y_train size: (19075,)\n",
      "\n",
      "validation_df size: (2384, 445)\n",
      "X_validation size: (2384, 444)\n",
      "y_validation size: (2384,)\n",
      "\n",
      "test_df size: (2385, 445)\n",
      "X_test size: (2385, 444)\n",
      "y_test size: (2385,)\n",
      "Loading data completed\n",
      "\n",
      "5-Preprocessing data...\n",
      "Preprocessing data completed.\n",
      "Processor saved successfully\n",
      "Length y_test 2385\n",
      "Count number of rows per classes of WAITTIME_BINARY\n",
      "0    17079\n",
      "1     6765\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the data preprocessing pipeline\n",
    "pipeline = DataPreprocessingPipeline(emergency_df=emergency_df,target=target,target_to_drop=target_to_drop, percent_train=0.80,percent_val=0.10,percent_test=0.10,path=path,stratify=True)\n",
    "\n",
    "# Run the pipeline\n",
    "pipeline.run()\n",
    "\n",
    "X_train = pipeline.X_train\n",
    "X_validation = pipeline.X_validation\n",
    "X_test = pipeline.X_test\n",
    "\n",
    "y_train = pipeline.y_train\n",
    "y_validation = pipeline.y_validation\n",
    "y_test = pipeline.y_test\n",
    "print(\"Length y_test\",len(y_test))\n",
    "\n",
    "X_train_preprocessed = pipeline.X_train_preprocessed\n",
    "X_validation_preprocessed = pipeline.X_validation_preprocessed\n",
    "X_test_preprocessed = pipeline.X_test_preprocessed\n",
    "\n",
    "feature_names = pipeline.feature_names\n",
    "\n",
    "emergency_df = pipeline.emergency_df \n",
    "cleaned_emergency_df = pipeline.cleaned_emergency_df \n",
    "transformed_emergency_df = pipeline.transformed_emergency_df\n",
    "\n",
    "print(\"Count number of rows per classes of\", cleaned_emergency_df[target].value_counts())\n",
    "\n",
    "preprocessor = pipeline.processor\n",
    "\n",
    "# Ensure feature_names is treated as a list\n",
    "feature_names = [name.replace('num__', '').replace('cat__', '') for name in feature_names]\n",
    "feature_names_list = list(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model and parameters for RandomizedSearchCV\n",
    "models_params = {\n",
    "     'LGBMClassifier': {\n",
    "        'model': LGBMClassifier(is_unbalance=True,verbose=-1),\n",
    "        'params': {\n",
    "            'classifier__max_depth': [3, 4, 5], \n",
    "            'classifier__learning_rate': [0.01, 0.1], \n",
    "            'classifier__n_estimators': [100, 200]\n",
    "        }\n",
    "    },\n",
    "     'CatBoostClassifier': {\n",
    "        'model': CatBoostClassifier(verbose=0),\n",
    "        'params': {\n",
    "            'classifier__depth': [3, 4, 5], \n",
    "            'classifier__learning_rate': [0.01, 0.1],\n",
    "            'classifier__iterations': [100, 200],\n",
    "            'classifier__train_dir': [gettempdir()] \n",
    "        }\n",
    "    },\n",
    "    'RandomForestClassifier': {\n",
    "        'model': RandomForestClassifier(),\n",
    "        'params': {\n",
    "            'classifier__n_estimators': [100, 200],\n",
    "            'classifier__max_depth': [None, 5, 10],\n",
    "            'classifier__min_samples_leaf': [1, 2, 4]\n",
    "        }\n",
    "    },\n",
    "    'XGBClassifier': {\n",
    "        'model': XGBClassifier(use_label_encoder=False, eval_metric='logloss'),\n",
    "        'params': {\n",
    "            'classifier__max_depth': [3, 5, 7],\n",
    "            'classifier__learning_rate': [0.01, 0.1, 0.2],\n",
    "            'classifier__n_estimators': [100, 500],\n",
    "            'classifier__subsample': [0.6, 0.8],\n",
    "            'classifier__colsample_bytree': [0.6, 0.8]\n",
    "        }\n",
    "    },\n",
    "    'Random Forest Classifier': {\n",
    "        'model': RandomForestClassifier(random_state=RANDOM_SEED),\n",
    "        'params': {\n",
    "            'classifier__n_estimators': [100, 200], \n",
    "            'classifier__max_depth': [10, 20, None]\n",
    "        }\n",
    "    },\n",
    "    'Gradient Boosting Classifier': {\n",
    "        'model': GradientBoostingClassifier(random_state=RANDOM_SEED),\n",
    "        'params': {\n",
    "            'classifier__n_estimators': [100, 200], \n",
    "            'classifier__learning_rate': [0.01, 0.1, 0.2], \n",
    "            'classifier__max_depth': [3, 5, 7]\n",
    "        }\n",
    "    }\n",
    "    ,\n",
    "    'XGBClassifier (Updated)': {\n",
    "        'model': XGBClassifier(use_label_encoder=False, eval_metric='logloss'),\n",
    "        'params': {\n",
    "            'classifier__max_depth': [3, 4, 5], \n",
    "            'classifier__learning_rate': [0.01, 0.1], \n",
    "            'classifier__n_estimators': [100, 200]\n",
    "        }\n",
    "    },\n",
    "    'MLPClassifier': {\n",
    "        'model': MLPClassifier(max_iter=100),\n",
    "        'params': {\n",
    "            'classifier__alpha': [0.0001, 0.001, 0.01], \n",
    "            'classifier__hidden_layer_sizes': [(100,), (50, 50), (100, 50)]\n",
    "        }\n",
    "    }\n",
    " \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n",
    "\n",
    "# from sklearn.model_selection import cross_validate\n",
    "\n",
    "# # Define scoring metrics\n",
    "# scoring_metrics = {\n",
    "#     'f1_weighted': 'f1_weighted',\n",
    "#     'roc_auc': 'roc_auc',\n",
    "#     'precision': 'precision_weighted',\n",
    "#     'recall': 'recall_weighted'\n",
    "# }\n",
    "\n",
    "# model_scores = {}\n",
    "\n",
    "# for model_name, mp in models_params.items():\n",
    "#     print(f\"Evaluating model: {model_name}\")\n",
    "#     pipeline = ImbPipeline([\n",
    "#         ('preprocessor', preprocessor),  # Ensure this is correctly defined\n",
    "#         ('smote', SMOTE(random_state=RANDOM_SEED)),\n",
    "#         ('classifier', mp['model'])\n",
    "#     ])\n",
    "    \n",
    "#     # Calculate cross-validated scores for multiple metrics\n",
    "#     scores = cross_validate(pipeline, X_train, y_train, scoring=scoring_metrics, cv=cv, n_jobs=-1, return_train_score=False)\n",
    "    \n",
    "#     # Compute the mean for each metric\n",
    "#     mean_scores = {metric: np.mean(scores[f'test_{metric}']) for metric in scoring_metrics}\n",
    "    \n",
    "#     model_scores[model_name] = mean_scores\n",
    "    \n",
    "#     print(f\"Model: {model_name} - Scores: {mean_scores}\")\n",
    "\n",
    "# # We prioritize F1, but also consider ROC-AUC, Precision, and Recall as secondary criteria\n",
    "# best_model_name = max(model_scores, key=lambda k: (\n",
    "#     model_scores[k]['f1_weighted'],\n",
    "#     model_scores[k]['roc_auc'],\n",
    "#     model_scores[k]['precision'],\n",
    "#     model_scores[k]['recall']\n",
    "# ))\n",
    "# print(f\"Best model based on comprehensive evaluation: {best_model_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model: LGBMClassifier\n",
      "Model: LGBMClassifier - Scores: {'f1_weighted': 0.6578240638545965, 'roc_auc': 0.694243591649756, 'precision': 0.6948898913385138, 'recall': 0.6422018348623852}\n",
      "Evaluating model: CatBoostClassifier\n",
      "Model: CatBoostClassifier - Scores: {'f1_weighted': 0.7663315032717346, 'roc_auc': 0.8294752911313983, 'precision': 0.768435948361739, 'recall': 0.7811795543905637}\n",
      "Evaluating model: RandomForestClassifier\n",
      "Model: RandomForestClassifier - Scores: {'f1_weighted': 0.6679801716251152, 'roc_auc': 0.7514411836852045, 'precision': 0.7222507952442855, 'recall': 0.7368807339449541}\n",
      "Evaluating model: XGBClassifier\n",
      "Model: XGBClassifier - Scores: {'f1_weighted': 0.7627181703045925, 'roc_auc': 0.8186460364550688, 'precision': 0.7617791121751609, 'recall': 0.7747313237221495}\n",
      "Evaluating model: Random Forest Classifier\n",
      "Model: Random Forest Classifier - Scores: {'f1_weighted': 0.6668856235393296, 'roc_auc': 0.7487316839163538, 'precision': 0.7232643621166495, 'recall': 0.7367758846657929}\n",
      "Evaluating model: Gradient Boosting Classifier\n",
      "Model: Gradient Boosting Classifier - Scores: {'f1_weighted': 0.7305307644971788, 'roc_auc': 0.7842976494754205, 'precision': 0.7427779607886837, 'recall': 0.76}\n",
      "Evaluating model: XGBClassifier (Updated)\n",
      "Model: XGBClassifier (Updated) - Scores: {'f1_weighted': 0.7627181703045925, 'roc_auc': 0.8186460364550688, 'precision': 0.7617791121751609, 'recall': 0.7747313237221495}\n",
      "Evaluating model: MLPClassifier\n",
      "Model: MLPClassifier - Scores: {'f1_weighted': 0.7221291075365776, 'roc_auc': 0.744045714875225, 'precision': 0.7181551387030022, 'recall': 0.728335517693316}\n",
      "Best model based on comprehensive evaluation: CatBoostClassifier\n"
     ]
    }
   ],
   "source": [
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier, EasyEnsembleClassifier\n",
    "import numpy as np\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n",
    "\n",
    "# Define scoring metrics\n",
    "scoring_metrics = {\n",
    "    'f1_weighted': 'f1_weighted',\n",
    "    'roc_auc': 'roc_auc',\n",
    "    'precision': 'precision_weighted',\n",
    "    'recall': 'recall_weighted'\n",
    "}\n",
    "\n",
    "model_scores = {}\n",
    "\n",
    "\n",
    "for model_name, mp in models_params.items():\n",
    "    print(f\"Evaluating model: {model_name}\")\n",
    "    \n",
    "    # Adjust the resampling technique or class weight here based on the model\n",
    "    if model_name in ['ModelNameSupportingClassWeight']:  # Placeholder for actual model name\n",
    "        classifier = mp['model']\n",
    "        classifier.set_params(**{'class_weight': 'balanced'})\n",
    "        pipeline_steps = [('preprocessor', preprocessor), ('classifier', classifier)]\n",
    "    else:\n",
    "        resampling = SMOTE(random_state=RANDOM_SEED)  # Or any other resampling strategy\n",
    "        pipeline_steps = [('preprocessor', preprocessor), ('resampling', resampling), ('classifier', mp['model'])]\n",
    "    \n",
    "    pipeline = ImbPipeline(steps=pipeline_steps)\n",
    "    \n",
    "    # Calculate cross-validated scores for multiple metrics\n",
    "    scores = cross_validate(pipeline, X_train, y_train, scoring=scoring_metrics, cv=cv, n_jobs=-1, return_train_score=False)\n",
    "    \n",
    "    # Compute the mean for each metric\n",
    "    mean_scores = {metric: np.mean(scores[f'test_{metric}']) for metric in scoring_metrics}\n",
    "    \n",
    "    model_scores[model_name] = mean_scores\n",
    "    \n",
    "    print(f\"Model: {model_name} - Scores: {mean_scores}\")\n",
    "\n",
    "# Determine the best model based on F1 score and secondary criteria\n",
    "best_model_name = max(model_scores, key=lambda k: (\n",
    "    model_scores[k]['f1_weighted'],\n",
    "    model_scores[k]['roc_auc'],\n",
    "    model_scores[k]['precision'],\n",
    "    model_scores[k]['recall']\n",
    "))\n",
    "print(f\"Best model based on comprehensive evaluation: {best_model_name}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emergency-dept-optimization-VxYDoEAm-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
