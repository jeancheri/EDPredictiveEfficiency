{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "import shap\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "# from sklearn.inspection import plot_partial_dependence\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "# Loading dataset\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "#  project root directory. .\n",
    "project_root = Path.cwd().parents[1]\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Import the project configuration settings.\n",
    "import config\n",
    "from config import RANDOM_SEED\n",
    "config.set_seed()\n",
    "# Import our custom utility functions\n",
    "from utils import DataPreprocessingPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the raw data from the data/raw directory using the load_data method from the config module located at config.org\n",
    "emergency_df = config.load_data('nhamcs14.sas7bdat', 'raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-Cleaning data...\n",
      "Data cleaning completed\n",
      "Size of Initial dataset:(23844, 1012)\n",
      "Size of cleaned dataset:(17959, 370)\n",
      "\n",
      "2-Applying feature engineering...\n",
      "Feature engineering completed\n",
      "Size of the dataset after feature engineering:(17959, 387)\n",
      "\n",
      "3-Splitting data...\n",
      "self.stratify: False\n",
      "Splitting data completed\n",
      "\n",
      "4-Loading data...\n",
      "train_df size: (12571, 387)\n",
      "X_train size: (12571, 386)\n",
      "y_train size: (12571,)\n",
      "\n",
      "validation_df size: (2693, 387)\n",
      "X_validation size: (2693, 386)\n",
      "y_validation size: (2693,)\n",
      "\n",
      "test_df size: (2695, 387)\n",
      "X_test size: (2695, 386)\n",
      "y_test size: (2695,)\n",
      "Loading data completed\n",
      "\n",
      "5-Preprocessing data...\n",
      "Preprocessing data completed.\n",
      "Processor saved successfully\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess data using a custom pipeline defined in the utils module.\n",
    "# Define the target variable and columns to drop\n",
    "target = \"LOV\"\n",
    "target_to_drop = ['LOV_BINARY', 'WAITTIME_BINARY']\n",
    "\n",
    "# Initializing the data preprocessing pipeline\n",
    "pipeline = DataPreprocessingPipeline(emergency_df=emergency_df, target=target,\n",
    "                                     target_to_drop=target_to_drop, percent_train=0.70,\n",
    "                                     percent_val=0.15, percent_test=0.15, stratify=False)\n",
    "\n",
    "# Running the data preprocessing steps\n",
    "pipeline.run()\n",
    "\n",
    "# Extracting the preprocessed training, validation, and test sets\n",
    "X_train_preprocessed = pipeline.X_train_preprocessed\n",
    "X_validation_preprocessed = pipeline.X_validation_preprocessed\n",
    "X_test_preprocessed = pipeline.X_test_preprocessed\n",
    "y_train = pipeline.y_train\n",
    "y_validation = pipeline.y_validation\n",
    "y_test = pipeline.y_test\n",
    "\n",
    "feature_names = pipeline.feature_names\n",
    "\n",
    "cleaned_emergency_df = pipeline.cleaned_emergency_df \n",
    "transformed_emergency_df = pipeline.transformed_emergency_df\n",
    "\n",
    "# Save the feature names for future reference\n",
    "feature_names = [name.replace('num__', '').replace('cat__', '') for name in pipeline.feature_names]\n",
    "feature_names_list = list(feature_names)\n",
    "config.save_data(feature_names, f\"features_{target}.csv\", 'features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Loads the tuned model and evaluates its performance on the test set\n",
    "model_train_dir = \"/Users/Macbook/Desktop/EDPredictiveEfficiency/notebooks/model_train/\"\n",
    "model_filename = \"CatBoostRegressor_tuned.joblib\"\n",
    "model_file_path = os.path.join(model_train_dir, model_filename)\n",
    "trained_model = joblib.load(model_file_path) if os.path.exists(model_file_path) else print(f\"No such file found: {model_file_path}\")\n",
    "print(\"Model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Performance (All features):\n",
      "Mean Absolute Error (MAE) on Test Data: 51.58\n",
      "R-squared on Test Data: 0.55\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test dataset with_all_features\n",
    "y_pred_test = trained_model.predict(X_test_preprocessed)\n",
    "# Evaluate the final model's performance on the test dataset\n",
    "mae_test = mean_absolute_error(y_test, y_pred_test)\n",
    "r2_test = r2_score(y_test, y_pred_test)\n",
    "print(\"Test Set Performance (All features):\")\n",
    "print(f\"Mean Absolute Error (MAE) on Test Data: {round(mae_test, 2)}\")\n",
    "print(f\"R-squared on Test Data: {round(r2_test, 2)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emergency-dept-optimization-QtjxB3YM-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
