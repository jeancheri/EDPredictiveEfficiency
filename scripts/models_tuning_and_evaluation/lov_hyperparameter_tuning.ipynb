{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import cross_val_score, cross_validate, GridSearchCV\n",
    "from catboost import CatBoostRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, make_scorer\n",
    "project_root = Path.cwd().parents[1]\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Import the project configuration settings.\n",
    "import config\n",
    "from config import RANDOM_SEED\n",
    "config.set_seed()\n",
    "# Import our custom utility functions\n",
    "from utils import DataPreprocessingPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the raw data from the data/raw directory using the load_data method from the config module located at config.org\n",
    "emergency_df = config.load_data('nhamcs14.sas7bdat', 'raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-Cleaning data...\n",
      "Data cleaning completed\n",
      "Size of Initial dataset:(23844, 1012)\n",
      "Size of cleaned dataset:(17959, 370)\n",
      "\n",
      "2-Applying feature engineering...\n",
      "Feature engineering completed\n",
      "Size of the dataset after feature engineering:(17959, 387)\n",
      "\n",
      "3-Splitting data...\n",
      "self.stratify: False\n",
      "Splitting data completed\n",
      "\n",
      "4-Loading data...\n",
      "train_df size: (12571, 387)\n",
      "X_train size: (12571, 386)\n",
      "y_train size: (12571,)\n",
      "\n",
      "validation_df size: (2693, 387)\n",
      "X_validation size: (2693, 386)\n",
      "y_validation size: (2693,)\n",
      "\n",
      "test_df size: (2695, 387)\n",
      "X_test size: (2695, 386)\n",
      "y_test size: (2695,)\n",
      "Loading data completed\n",
      "\n",
      "5-Preprocessing data...\n",
      "Preprocessing data completed.\n",
      "Processor saved successfully\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess data using a custom pipeline defined in the utils module.\n",
    "# Define the target variable and columns to drop\n",
    "target = \"LOV\"\n",
    "target_to_drop = ['LOV_BINARY', 'WAITTIME_BINARY']\n",
    "\n",
    "# Initializing the data preprocessing pipeline\n",
    "pipeline = DataPreprocessingPipeline(emergency_df=emergency_df, target=target,\n",
    "                                     target_to_drop=target_to_drop, percent_train=0.70,\n",
    "                                     percent_val=0.15, percent_test=0.15, stratify=False)\n",
    "\n",
    "# Running the data preprocessing steps\n",
    "pipeline.run()\n",
    "\n",
    "# Extracting the preprocessed training, validation, and test sets\n",
    "X_train_preprocessed = pipeline.X_train_preprocessed\n",
    "X_validation_preprocessed = pipeline.X_validation_preprocessed\n",
    "X_test_preprocessed = pipeline.X_test_preprocessed\n",
    "y_train = pipeline.y_train\n",
    "y_validation = pipeline.y_validation\n",
    "y_test = pipeline.y_test\n",
    "\n",
    "feature_names = pipeline.feature_names\n",
    "\n",
    "cleaned_emergency_df = pipeline.cleaned_emergency_df \n",
    "transformed_emergency_df = pipeline.transformed_emergency_df\n",
    "\n",
    "# Save the feature names for future reference\n",
    "feature_names = [name.replace('num__', '').replace('cat__', '') for name in pipeline.feature_names]\n",
    "feature_names_list = list(feature_names)\n",
    "config.save_data(feature_names, f\"features_{target}.csv\", 'features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the tuned model and evaluates its performance on the test set\n",
    "model_train_dir = \"/Users/Macbook/Desktop/EDPredictiveEfficiency/scripts/models_training_and_selection/model_train/\"\n",
    "model_filename = \"CatBoostRegressor.joblib\"\n",
    "model_file_path = os.path.join(model_train_dir, model_filename)\n",
    "best_trained_model = joblib.load(model_file_path) if os.path.exists(model_file_path) else print(f\"No such file found: {model_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing a ModelTuner class to tune the hyperparameters of the best model using GridSearchCV\n",
    "# Defining the hyperparameter grid and scoring metric for tuning\n",
    "# Tuneing the hyperparameters, evaluates the best model on the validation set, and saves the tuned model\n",
    "class ModelTuner:\n",
    "    def __init__(self, model, param_grid, scoring, cv=5, n_jobs=-1, verbose=2):\n",
    "        self.model = model\n",
    "        self.param_grid = param_grid\n",
    "        self.scoring = scoring\n",
    "        self.cv = cv\n",
    "        self.n_jobs = n_jobs\n",
    "        self.verbose = verbose\n",
    "        self.grid_search = None\n",
    "        self.best_model = None\n",
    "\n",
    "    def tune_hyperparameters(self, X_train, y_train):\n",
    "        \"\"\"Tune hyperparameters using GridSearchCV.\"\"\"\n",
    "        self.grid_search = GridSearchCV(\n",
    "            estimator=self.model,\n",
    "            param_grid=self.param_grid,\n",
    "            cv=self.cv,\n",
    "            scoring=self.scoring,\n",
    "            refit=True,\n",
    "            verbose=self.verbose,\n",
    "            n_jobs=self.n_jobs\n",
    "        )\n",
    "        self.grid_search.fit(X_train, y_train)\n",
    "        self.best_model = self.grid_search.best_estimator_\n",
    "        print(\"Best Hyperparameters:\", self.grid_search.best_params_)\n",
    "        print(\"Best Cross-Validation MAE:\", -self.grid_search.best_score_)\n",
    "\n",
    "    def evaluate(self, X_val, y_val):\n",
    "        \"\"\"Evaluate the best model on the validation set.\"\"\"\n",
    "        y_pred_validation = self.best_model.predict(X_val)\n",
    "        mae_validation, r2_validation = mean_absolute_error(y_val, y_pred_validation), r2_score(y_val, y_pred_validation)\n",
    "        print(f\"Validation Set Performance:\\nMAE: {mae_validation}\\nR-squared: {r2_validation}\")\n",
    "\n",
    "    def save_model(self, model_directory, model_name):\n",
    "        \"\"\"Save the best model to the specified directory.\"\"\"\n",
    "        joblib.dump(self.best_model, os.path.join(model_directory, model_name))\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'depth': [6, 8],\n",
    "    'learning_rate': [0.01, 0.03, 0.1],\n",
    "    'iterations': [1000],\n",
    "    'l2_leaf_reg': [1, 3, 4]\n",
    "}\n",
    "mae_scorer = make_scorer(mean_absolute_error, greater_is_better=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "[CV] END depth=6, iterations=1000, l2_leaf_reg=1, learning_rate=0.03; total time= 1.1min\n",
      "[CV] END depth=6, iterations=1000, l2_leaf_reg=1, learning_rate=0.03; total time= 1.1min\n",
      "[CV] END depth=6, iterations=1000, l2_leaf_reg=1, learning_rate=0.03; total time= 1.1min\n",
      "[CV] END depth=6, iterations=1000, l2_leaf_reg=1, learning_rate=0.01; total time= 1.1min\n",
      "[CV] END depth=6, iterations=1000, l2_leaf_reg=1, learning_rate=0.01; total time= 1.1min\n",
      "[CV] END depth=6, iterations=1000, l2_leaf_reg=1, learning_rate=0.01; total time= 1.1min\n",
      "[CV] END depth=6, iterations=1000, l2_leaf_reg=1, learning_rate=0.01; total time= 1.2min\n",
      "[CV] END depth=6, iterations=1000, l2_leaf_reg=1, learning_rate=0.01; total time= 1.2min\n",
      "[CV] END depth=6, iterations=1000, l2_leaf_reg=1, learning_rate=0.03; total time= 1.1min\n",
      "[CV] END depth=6, iterations=1000, l2_leaf_reg=1, learning_rate=0.03; total time= 1.1min\n",
      "[CV] END depth=6, iterations=1000, l2_leaf_reg=1, learning_rate=0.1; total time= 1.1min\n",
      "[CV] END depth=6, iterations=1000, l2_leaf_reg=1, learning_rate=0.1; total time= 1.1min\n",
      "[CV] END depth=6, iterations=1000, l2_leaf_reg=1, learning_rate=0.1; total time= 1.1min\n",
      "[CV] END depth=6, iterations=1000, l2_leaf_reg=1, learning_rate=0.1; total time= 1.1min\n",
      "[CV] END depth=6, iterations=1000, l2_leaf_reg=1, learning_rate=0.1; total time= 1.1min\n",
      "[CV] END depth=6, iterations=1000, l2_leaf_reg=3, learning_rate=0.01; total time= 1.1min\n",
      "[CV] END depth=6, iterations=1000, l2_leaf_reg=3, learning_rate=0.03; total time= 1.3min\n",
      "[CV] END depth=6, iterations=1000, l2_leaf_reg=3, learning_rate=0.01; total time= 1.3min\n",
      "[CV] END depth=6, iterations=1000, l2_leaf_reg=3, learning_rate=0.01; total time= 1.3min\n",
      "[CV] END depth=6, iterations=1000, l2_leaf_reg=3, learning_rate=0.01; total time= 1.3min\n",
      "[CV] END depth=6, iterations=1000, l2_leaf_reg=3, learning_rate=0.03; total time= 1.3min\n",
      "[CV] END depth=6, iterations=1000, l2_leaf_reg=3, learning_rate=0.03; total time= 1.3min\n",
      "[CV] END depth=6, iterations=1000, l2_leaf_reg=3, learning_rate=0.03; total time= 1.3min\n",
      "[CV] END depth=6, iterations=1000, l2_leaf_reg=3, learning_rate=0.01; total time= 1.3min\n",
      "[CV] END depth=6, iterations=1000, l2_leaf_reg=3, learning_rate=0.03; total time= 1.2min\n",
      "[CV] END depth=6, iterations=1000, l2_leaf_reg=3, learning_rate=0.1; total time= 1.2min\n",
      "[CV] END depth=6, iterations=1000, l2_leaf_reg=3, learning_rate=0.1; total time= 1.2min\n",
      "[CV] END depth=6, iterations=1000, l2_leaf_reg=3, learning_rate=0.1; total time= 1.2min\n",
      "[CV] END depth=6, iterations=1000, l2_leaf_reg=3, learning_rate=0.1; total time= 1.2min\n",
      "[CV] END depth=6, iterations=1000, l2_leaf_reg=3, learning_rate=0.1; total time= 1.2min\n",
      "[CV] END depth=6, iterations=1000, l2_leaf_reg=4, learning_rate=0.01; total time= 1.2min\n",
      "[CV] END depth=6, iterations=1000, l2_leaf_reg=4, learning_rate=0.01; total time= 1.2min\n",
      "[CV] END depth=6, iterations=1000, l2_leaf_reg=4, learning_rate=0.03; total time= 1.3min\n",
      "[CV] END depth=6, iterations=1000, l2_leaf_reg=4, learning_rate=0.03; total time= 1.3min\n",
      "[CV] END depth=6, iterations=1000, l2_leaf_reg=4, learning_rate=0.03; total time= 1.3min\n",
      "[CV] END depth=6, iterations=1000, l2_leaf_reg=4, learning_rate=0.01; total time= 1.3min\n",
      "[CV] END depth=6, iterations=1000, l2_leaf_reg=4, learning_rate=0.01; total time= 1.3min\n",
      "[CV] END depth=6, iterations=1000, l2_leaf_reg=4, learning_rate=0.01; total time= 1.3min\n",
      "[CV] END depth=6, iterations=1000, l2_leaf_reg=4, learning_rate=0.03; total time= 1.3min\n",
      "[CV] END depth=6, iterations=1000, l2_leaf_reg=4, learning_rate=0.03; total time= 1.3min\n",
      "[CV] END depth=6, iterations=1000, l2_leaf_reg=4, learning_rate=0.1; total time= 1.6min\n",
      "[CV] END depth=6, iterations=1000, l2_leaf_reg=4, learning_rate=0.1; total time= 1.6min\n",
      "[CV] END depth=6, iterations=1000, l2_leaf_reg=4, learning_rate=0.1; total time= 1.6min\n",
      "[CV] END depth=6, iterations=1000, l2_leaf_reg=4, learning_rate=0.1; total time= 1.6min\n",
      "[CV] END depth=6, iterations=1000, l2_leaf_reg=4, learning_rate=0.1; total time= 1.6min\n",
      "[CV] END depth=8, iterations=1000, l2_leaf_reg=1, learning_rate=0.01; total time= 3.8min\n",
      "[CV] END depth=8, iterations=1000, l2_leaf_reg=1, learning_rate=0.01; total time= 3.8min\n",
      "[CV] END depth=8, iterations=1000, l2_leaf_reg=1, learning_rate=0.01; total time= 3.8min\n",
      "[CV] END depth=8, iterations=1000, l2_leaf_reg=1, learning_rate=0.03; total time= 4.0min\n",
      "[CV] END depth=8, iterations=1000, l2_leaf_reg=1, learning_rate=0.01; total time= 4.0min\n",
      "[CV] END depth=8, iterations=1000, l2_leaf_reg=1, learning_rate=0.01; total time= 4.0min\n",
      "[CV] END depth=8, iterations=1000, l2_leaf_reg=1, learning_rate=0.03; total time= 4.0min\n",
      "[CV] END depth=8, iterations=1000, l2_leaf_reg=1, learning_rate=0.03; total time= 4.0min\n",
      "[CV] END depth=8, iterations=1000, l2_leaf_reg=1, learning_rate=0.03; total time= 3.8min\n",
      "[CV] END depth=8, iterations=1000, l2_leaf_reg=1, learning_rate=0.1; total time= 3.8min\n",
      "[CV] END depth=8, iterations=1000, l2_leaf_reg=1, learning_rate=0.03; total time= 3.8min\n",
      "[CV] END depth=8, iterations=1000, l2_leaf_reg=1, learning_rate=0.1; total time= 3.9min\n",
      "[CV] END depth=8, iterations=1000, l2_leaf_reg=1, learning_rate=0.1; total time= 3.9min\n",
      "[CV] END depth=8, iterations=1000, l2_leaf_reg=1, learning_rate=0.1; total time= 3.9min\n",
      "[CV] END depth=8, iterations=1000, l2_leaf_reg=1, learning_rate=0.1; total time= 3.9min\n",
      "[CV] END depth=8, iterations=1000, l2_leaf_reg=3, learning_rate=0.01; total time= 3.9min\n",
      "[CV] END depth=8, iterations=1000, l2_leaf_reg=3, learning_rate=0.01; total time= 4.0min\n",
      "[CV] END depth=8, iterations=1000, l2_leaf_reg=3, learning_rate=0.01; total time= 4.0min\n",
      "[CV] END depth=8, iterations=1000, l2_leaf_reg=3, learning_rate=0.01; total time= 4.1min\n",
      "[CV] END depth=8, iterations=1000, l2_leaf_reg=3, learning_rate=0.03; total time= 4.0min\n",
      "[CV] END depth=8, iterations=1000, l2_leaf_reg=3, learning_rate=0.03; total time= 4.0min\n",
      "[CV] END depth=8, iterations=1000, l2_leaf_reg=3, learning_rate=0.03; total time= 4.0min\n",
      "[CV] END depth=8, iterations=1000, l2_leaf_reg=3, learning_rate=0.01; total time= 4.0min\n",
      "[CV] END depth=8, iterations=1000, l2_leaf_reg=3, learning_rate=0.03; total time= 4.0min\n",
      "[CV] END depth=8, iterations=1000, l2_leaf_reg=3, learning_rate=0.03; total time= 4.3min\n",
      "[CV] END depth=8, iterations=1000, l2_leaf_reg=3, learning_rate=0.1; total time= 4.3min\n",
      "[CV] END depth=8, iterations=1000, l2_leaf_reg=3, learning_rate=0.1; total time= 4.3min\n",
      "[CV] END depth=8, iterations=1000, l2_leaf_reg=3, learning_rate=0.1; total time= 4.8min\n",
      "[CV] END depth=8, iterations=1000, l2_leaf_reg=3, learning_rate=0.1; total time= 4.8min\n",
      "[CV] END depth=8, iterations=1000, l2_leaf_reg=3, learning_rate=0.1; total time= 4.8min\n",
      "[CV] END depth=8, iterations=1000, l2_leaf_reg=4, learning_rate=0.01; total time= 4.8min\n",
      "[CV] END depth=8, iterations=1000, l2_leaf_reg=4, learning_rate=0.01; total time= 4.9min\n",
      "[CV] END depth=8, iterations=1000, l2_leaf_reg=4, learning_rate=0.01; total time= 4.9min\n",
      "[CV] END depth=8, iterations=1000, l2_leaf_reg=4, learning_rate=0.01; total time= 4.9min\n",
      "[CV] END depth=8, iterations=1000, l2_leaf_reg=4, learning_rate=0.01; total time= 4.8min\n",
      "[CV] END depth=8, iterations=1000, l2_leaf_reg=4, learning_rate=0.03; total time= 4.6min\n",
      "[CV] END depth=8, iterations=1000, l2_leaf_reg=4, learning_rate=0.03; total time= 4.6min\n",
      "[CV] END depth=8, iterations=1000, l2_leaf_reg=4, learning_rate=0.03; total time= 4.6min\n",
      "[CV] END depth=8, iterations=1000, l2_leaf_reg=4, learning_rate=0.03; total time= 4.6min\n",
      "[CV] END depth=8, iterations=1000, l2_leaf_reg=4, learning_rate=0.03; total time= 4.6min\n",
      "[CV] END depth=8, iterations=1000, l2_leaf_reg=4, learning_rate=0.1; total time= 3.7min\n",
      "[CV] END depth=8, iterations=1000, l2_leaf_reg=4, learning_rate=0.1; total time= 3.7min\n",
      "[CV] END depth=8, iterations=1000, l2_leaf_reg=4, learning_rate=0.1; total time= 3.7min\n",
      "[CV] END depth=8, iterations=1000, l2_leaf_reg=4, learning_rate=0.1; total time= 2.2min\n",
      "[CV] END depth=8, iterations=1000, l2_leaf_reg=4, learning_rate=0.1; total time= 2.2min\n",
      "Best Hyperparameters: {'depth': 8, 'iterations': 1000, 'l2_leaf_reg': 1, 'learning_rate': 0.03}\n",
      "Best Cross-Validation MAE: 51.03501500947977\n",
      "Validation Set Performance:\n",
      "MAE: 50.76539209096085\n",
      "R-squared: 0.5415707842188555\n"
     ]
    }
   ],
   "source": [
    "# Tune hyperparameters, evaluate the best model, and save it\n",
    "tuner = ModelTuner(best_trained_model, param_grid, scoring=mae_scorer)\n",
    "tuner.tune_hyperparameters(X_train_preprocessed, y_train)\n",
    "tuner.evaluate(X_validation_preprocessed, y_validation)\n",
    "model_directory = \"/Users/Macbook/Desktop/EDPredictiveEfficiency/notebooks/model_train\"\n",
    "tuner.save_model(model_directory, \"CatBoostRegressor_tuned.joblib\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edpredictiveefficiency-6OWGp70p-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
